{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "import langid\n",
    "import preprocessor as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if string is blank\n",
    "def check_blanks(data_str):\n",
    "    is_blank = str(data_str.isspace())\n",
    "    return is_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if language of text is english or not\n",
    "def check_lang(data_str):\n",
    "    predict_lang = langid.classify(data_str)\n",
    "    if predict_lang[1] >= .8:\n",
    "        language = predict_lang[0]\n",
    "    else:\n",
    "        language = 'NA'\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted chars\n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove @mentions\n",
    "    data_str = mention_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 3 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words removal\n",
    "def remove_stops(data_str):\n",
    "    # expects a string\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    text = data_str.split()\n",
    "    for word in text:\n",
    "        if word not in stops:\n",
    "            # rebuild cleaned_str\n",
    "            if list_pos == 0:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            list_pos += 1\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagging text\n",
    "def tag_and_remove(data_str):\n",
    "    cleaned_str = ' '\n",
    "    # noun tags\n",
    "    nn_tags = ['NN', 'NNP', 'NNP', 'NNPS', 'NNS']\n",
    "    # adjectives\n",
    "    jj_tags = ['JJ', 'JJR', 'JJS']\n",
    "    # verbs\n",
    "    vb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    nltk_tags = nn_tags + jj_tags + vb_tags\n",
    "\n",
    "    # break string into 'words'\n",
    "    text = data_str.split()\n",
    "\n",
    "    # tag the text and keep only those with the right tags\n",
    "    tagged_text = pos_tag(text)\n",
    "    for tagged_word in tagged_text:\n",
    "        if tagged_word[1] in nltk_tags:\n",
    "            cleaned_str += tagged_word[0] + ' '\n",
    "\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "def lemmatize(data_str):\n",
    "    # expects a string\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    text = data_str.split()\n",
    "    tagged_words = pos_tag(text)\n",
    "    for word in tagged_words:\n",
    "        if 'v' in word[1].lower():\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='v')\n",
    "        else:\n",
    "            lemma = lmtzr.lemmatize(word[0], pos='n')\n",
    "        if list_pos == 0:\n",
    "            cleaned_str = lemma\n",
    "        else:\n",
    "            cleaned_str = cleaned_str + ' ' + lemma\n",
    "        list_pos += 1\n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register all the functions in Preproc with Spark Context\n",
    "check_lang_udf = udf(check_lang, StringType())\n",
    "remove_stops_udf = udf(remove_stops, StringType())\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(lemmatize, StringType())\n",
    "check_blanks_udf = udf(check_blanks, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# create spark contexts\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "data_rdd = sc.textFile(\"../data/raw_data.txt\")\n",
    "parts_rdd = data_rdd.map(lambda l: l.split(\"\\t\"))\n",
    "\n",
    "# Filter bad rows out\n",
    "garantee_col_rdd = parts_rdd.filter(lambda l: len(l) == 3)\n",
    "typed_rdd = garantee_col_rdd.map(lambda p: (p[0], p[1], float(p[2])))\n",
    "\n",
    "#Create DataFrame\n",
    "data_df = sqlContext.createDataFrame(typed_rdd, [\"text\", \"id\", \"label\"])\n",
    "\n",
    "# get the raw columns\n",
    "raw_cols = data_df.columns\n",
    "\n",
    "#data_df.show()\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+\n",
      "|                text|                id|label|\n",
      "+--------------------+------------------+-----+\n",
      "|Fresh install of ...|        1018769417|  1.0|\n",
      "|Well. Now I know ...|       10284216536|  1.0|\n",
      "|\"Literally six we...|       10298589026|  1.0|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|\n",
      "|'Cheap Eats in SL...|109642968603963392|  1.0|\n",
      "+--------------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+----+\n",
      "|text| id|label|lang|\n",
      "+----+---+-----+----+\n",
      "+----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lang_df = data_df.withColumn(\"lang\", check_lang_udf(data_df[\"text\"]))\n",
    "en_df = lang_df.filter(lang_df[\"lang\"] == \"en\")\n",
    "en_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+----+\n",
      "|                text|                id|label|lang|\n",
      "+--------------------+------------------+-----+----+\n",
      "|Fresh install of ...|        1018769417|  1.0|  NA|\n",
      "|Well. Now I know ...|       10284216536|  1.0|  NA|\n",
      "|\"Literally six we...|       10298589026|  1.0|  NA|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|  NA|\n",
      "|'Cheap Eats in SL...|109642968603963392|  1.0|  NA|\n",
      "+--------------------+------------------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lang_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+--------------------+\n",
      "|                text|                id|label|           stop_text|\n",
      "+--------------------+------------------+-----+--------------------+\n",
      "|Fresh install of ...|        1018769417|  1.0|Fresh install XP ...|\n",
      "|Well. Now I know ...|       10284216536|  1.0|Well. Now I know ...|\n",
      "|\"Literally six we...|       10298589026|  1.0|\"Literally six we...|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|Mitsubishi MiEV -...|\n",
      "+--------------------+------------------+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rm_stops_df = data_df.select(raw_cols)\\\n",
    "                   .withColumn(\"stop_text\", remove_stops_udf(en_df[\"text\"]))\n",
    "rm_stops_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+--------------------+--------------------+\n",
      "|                text|                id|label|           stop_text|           feat_text|\n",
      "+--------------------+------------------+-----+--------------------+--------------------+\n",
      "|Fresh install of ...|        1018769417|  1.0|Fresh install XP ...|fresh install  ne...|\n",
      "|Well. Now I know ...|       10284216536|  1.0|Well. Now I know ...|well now  know   ...|\n",
      "|\"Literally six we...|       10298589026|  1.0|\"Literally six we...|literally six wee...|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|Mitsubishi MiEV -...|mitsubishi miev w...|\n",
      "+--------------------+------------------+-----+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rm_features_df = rm_stops_df.select(raw_cols+[\"stop_text\"])\\\n",
    "                            .withColumn(\"feat_text\", \\\n",
    "                            remove_features_udf(rm_stops_df[\"stop_text\"]))\n",
    "rm_features_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+--------------------+--------------------+\n",
      "|                text|                id|label|           feat_text|         tagged_text|\n",
      "+--------------------+------------------+-----+--------------------+--------------------+\n",
      "|Fresh install of ...|        1018769417|  1.0|fresh install  ne...| fresh install ne...|\n",
      "|Well. Now I know ...|       10284216536|  1.0|well now  know   ...| know want knives...|\n",
      "|\"Literally six we...|       10298589026|  1.0|literally six wee...| weeks take ssc c...|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|mitsubishi miev w...| mitsubishi miev ...|\n",
      "+--------------------+------------------+-----+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagged_df = rm_features_df.select(raw_cols+[\"feat_text\"]) \\\n",
    "                          .withColumn(\"tagged_text\", \\\n",
    "                           tag_and_remove_udf(rm_features_df.feat_text))\n",
    "\n",
    "tagged_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+--------------------+--------------------+\n",
      "|                text|                id|label|         tagged_text|           lemm_text|\n",
      "+--------------------+------------------+-----+--------------------+--------------------+\n",
      "|Fresh install of ...|        1018769417|  1.0| fresh install ne...|fresh install new...|\n",
      "|Well. Now I know ...|       10284216536|  1.0| know want knives...|know want knife c...|\n",
      "|\"Literally six we...|       10298589026|  1.0| weeks take ssc c...|week take ssc cha...|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0| mitsubishi miev ...|mitsubishi miev w...|\n",
      "+--------------------+------------------+-----+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemm_df = tagged_df.select(raw_cols+[\"tagged_text\"]) \\\n",
    "                   .withColumn(\"lemm_text\", lemmatize_udf(tagged_df[\"tagged_text\"]))\n",
    "lemm_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+--------+\n",
      "|                text|         id|label|           lemm_text|is_blank|\n",
      "+--------------------+-----------+-----+--------------------+--------+\n",
      "|Fresh install of ...| 1018769417|  1.0|fresh install new...|   False|\n",
      "|\"Did I really nee...|12358025545|  1.0|do need learn bou...|   False|\n",
      "|\"Literally six we...|10298589026|  1.0|week take ssc cha...|   False|\n",
      "|hi all - i'm goin...| 1208319583|  1.0|go tweet thing lo...|   False|\n",
      "+--------------------+-----------+-----+--------------------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove blank rows and duplicates\n",
    "check_blanks_df = lemm_df.select(raw_cols+[\"lemm_text\"])\\\n",
    "                             .withColumn(\"is_blank\", check_blanks_udf(lemm_df[\"lemm_text\"]))\n",
    "# remove blanks\n",
    "no_blanks_df = check_blanks_df.filter(check_blanks_df[\"is_blank\"] == \"False\")\n",
    "\n",
    "# drop duplicates\n",
    "dedup_df = no_blanks_df.dropDuplicates(['text', 'label'])\n",
    "\n",
    "dedup_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+--------+------------+\n",
      "|                text|         id|label|           lemm_text|is_blank|         uid|\n",
      "+--------------------+-----------+-----+--------------------+--------+------------+\n",
      "|Fresh install of ...| 1018769417|  1.0|fresh install new...|   False|231928233984|\n",
      "|\"Did I really nee...|12358025545|  1.0|do need learn bou...|   False|343597383680|\n",
      "|\"Literally six we...|10298589026|  1.0|week take ssc cha...|   False|695784701952|\n",
      "|hi all - i'm goin...| 1208319583|  1.0|go tweet thing lo...|   False|927712935936|\n",
      "+--------------------+-----------+-----+--------------------+--------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add unique id\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "# Create Unique ID\n",
    "dedup_df = dedup_df.withColumn(\"uid\", monotonically_increasing_id())\n",
    "dedup_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------------+-----+\n",
      "|         uid|         id|                text|label|\n",
      "+------------+-----------+--------------------+-----+\n",
      "|231928233984| 1018769417|Fresh install of ...|  1.0|\n",
      "|343597383680|12358025545|\"Did I really nee...|  1.0|\n",
      "|695784701952|10298589026|\"Literally six we...|  1.0|\n",
      "|927712935936| 1208319583|hi all - i'm goin...|  1.0|\n",
      "+------------+-----------+--------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating final data\n",
    "data = dedup_df.select('uid','id', 'text','label')\n",
    "data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (40% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "# vectorizer = CountVectorizer(inputCol= \"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(minDocFreq=3, inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Naive Bayes model\n",
    "nb = NaiveBayes()\n",
    "\n",
    "# Pipeline Architecture\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+-----+----------+\n",
      "|text                                                                                 |label|prediction|\n",
      "+-------------------------------------------------------------------------------------+-----+----------+\n",
      "|Fresh install of XP on new computer. Sweet relief! fuck vista                        |1.0  |0.0       |\n",
      "|'Cheap Eats in SLP' - http://t.co/4w8gRp7                                            |1.0  |0.0       |\n",
      "|Well. Now I know where to go when I want my knives. #ChiChevySXSW http://post.ly/RvDl|1.0  |0.0       |\n",
      "|Teenage Mutant Ninja Turtle art is never a bad thing... http://bit.ly/aDMHyW         |1.0  |0.0       |\n",
      "+-------------------------------------------------------------------------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"text\", \"label\", \"prediction\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "    \n",
    "https://runawayhorse001.github.io/LearningApacheSpark/textmining.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
